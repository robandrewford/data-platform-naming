# Naming Patterns Configuration Example
# This file defines naming pattern templates with placeholder variables
# Variables are substituted from naming-values.yaml (or inline overrides)

version: "1.0"

# Naming patterns for each resource type
# Use {variable_name} for placeholders that will be substituted
patterns:
  # AWS Resources
  aws_s3_bucket: "{project}-{purpose}-{layer}-{environment}-{region_short}"
  # Example: dataplatform-raw-raw-prd-use1
  
  aws_glue_database: "{project}_{domain}_{layer}_{environment}"
  # Example: dataplatform_finance_gold_prd
  
  aws_glue_table: "{table_type}_{entity}"
  # Example: fact_sales or dim_customers
  
  aws_glue_crawler: "{project}-{environment}-crawler-{database}-{source}"
  # Example: dataplatform-prd-crawler-sales-s3
  
  aws_lambda_function: "{project}-{environment}-{domain}-{trigger}-{action}"
  # Example: dataplatform-prd-sales-s3-process
  
  aws_iam_role: "{project}-{environment}-{service}-{purpose}-role"
  # Example: dataplatform-prd-lambda-execution-role
  
  aws_iam_policy: "{project}-{environment}-{service}-{purpose}-policy"
  # Example: dataplatform-prd-s3-read-policy
  
  aws_kinesis_stream: "{project}-{environment}-{domain}-{source}-stream"
  # Example: dataplatform-prd-events-api-stream
  
  aws_kinesis_firehose: "{project}-{environment}-{domain}-to-{destination}"
  # Example: dataplatform-prd-logs-to-s3
  
  aws_dynamodb_table: "{project}-{environment}-{entity}-{purpose}"
  # Example: dataplatform-prd-session-cache
  
  aws_sns_topic: "{project}-{environment}-{event_type}-{purpose}"
  # Example: dataplatform-prd-data-processed
  
  aws_sqs_queue: "{project}-{environment}-{purpose}-{queue_type}"
  # Example: dataplatform-prd-processing-fifo
  
  aws_step_function: "{project}-{environment}-{workflow}-{purpose}"
  # Example: dataplatform-prd-etl-orchestration
  
  # Databricks Resources
  dbx_workspace: "dbx-{project}-{purpose}-{environment}-{region_short}"
  # Example: dbx-dataplatform-analytics-prd-use1
  
  dbx_cluster: "{project}-{workload}-{cluster_type}-{environment}"
  # Example: dataplatform-etl-shared-prd
  
  dbx_job: "{project}-{job_type}-{purpose}-{schedule}-{environment}"
  # Example: dataplatform-batch-transformation-daily-prd
  
  dbx_notebook_path: "/{project}/{domain}/{purpose}/{environment}/{notebook_name}"
  # Example: /dataplatform/finance/etl/prd/customer-load
  
  dbx_repo: "{project}-{repo_purpose}-{environment}"
  # Example: dataplatform-etl-prd
  
  dbx_pipeline: "{project}-{pipeline_type}-{source}-{target}-{environment}"
  # Example: dataplatform-dlt-kafka-events-prd
  
  dbx_sql_warehouse: "{project}-sql-{purpose}-{size}-{environment}"
  # Example: dataplatform-sql-analytics-medium-prd
  
  # Unity Catalog (3-tier namespace)
  dbx_catalog: "{project}_{catalog_type}_{environment}"
  # Example: dataplatform_main_prd
  
  dbx_schema: "{domain}_{layer}"
  # Example: finance_gold
  
  dbx_table: "{table_type}_{entity}"
  # Example: fact_sales or dim_customers
  
  dbx_volume: "{data_type}_{purpose}"
  # Example: raw_landing
  
  dbx_secret_scope: "{project}-{purpose}-{environment}"
  # Example: dataplatform-aws-prd
  
  dbx_instance_pool: "{project}-pool-{node_type}-{purpose}-{environment}"
  # Example: dataplatform-pool-compute-etl-prd
  
  dbx_policy: "{project}-{target}-{policy_type}-{environment}"
  # Example: dataplatform-cluster-security-prd

# Optional transformations applied to variables before substitution
transformations:
  # Map full AWS region names to abbreviated codes
  # These match the REGION_CODES from aws_naming.py
  region_mapping:
    # US Regions
    us-east-1: use1
    us-east-2: use2
    us-west-1: usw1
    us-west-2: usw2
    # EU Regions
    eu-west-1: euw1
    eu-west-2: euw2
    eu-central-1: euc1
    # Asia Pacific Regions
    ap-southeast-1: aps1
    ap-southeast-2: aps2
    ap-northeast-1: apne1
  
  # Variables to automatically convert to lowercase
  lowercase:
    - project
    - environment
    - domain
    - layer
    - purpose
    - workload
  
  # Variables to convert to UPPERCASE (if needed)
  # uppercase:
  #   - cost_center
  
  # Replace hyphens with another character for specific variables
  # Useful when resource type requires underscores instead of hyphens
  replace_hyphens:
    project: "_"  # Convert project hyphens to underscores in patterns that use underscores
  
  # Hash generation configuration for unique suffixes
  # Used when include_hash=True in resource generation
  hash_generation:
    algorithm: md5      # Hash algorithm: 'md5' or 'sha256'
    length: 8           # Number of hash characters to include
    prefix: ""          # Optional prefix (e.g., 'h' for 'h12345678')
    separator: "-"      # Separator between name and hash

# Validation rules for generated names
validation:
  # Maximum length constraints per resource type
  # These match MAX_LENGTHS from aws_naming.py and dbx_naming.py
  max_length:
    # AWS Resources
    aws_s3_bucket: 63
    aws_glue_database: 255
    aws_glue_table: 255
    aws_glue_crawler: 255
    aws_lambda_function: 64
    aws_iam_role: 64
    aws_iam_policy: 128
    aws_kinesis_stream: 128
    aws_kinesis_firehose: 64
    aws_dynamodb_table: 255
    aws_sns_topic: 256
    aws_sqs_queue: 80
    aws_step_function: 80
    # Databricks Resources
    dbx_workspace: 100
    dbx_cluster: 100
    dbx_job: 200
    dbx_notebook_path: 512
    dbx_repo: 100
    dbx_pipeline: 200
    dbx_sql_warehouse: 128
    dbx_catalog: 255
    dbx_schema: 255
    dbx_table: 255
    dbx_volume: 255
    dbx_secret_scope: 128
    dbx_instance_pool: 128
    dbx_policy: 128
  
  # Allowed character patterns (regex) per resource type
  allowed_chars:
    aws_s3_bucket: "^[a-z0-9-]+$"          # Only lowercase alphanumeric and hyphens
    aws_glue_database: "^[a-z0-9_]+$"      # Only lowercase alphanumeric and underscores
    aws_glue_table: "^[a-z0-9_]+$"         # Only lowercase alphanumeric and underscores
    dbx_cluster: "^[a-zA-Z0-9-_]+$"        # Alphanumeric, hyphens, underscores
    dbx_job: "^[a-zA-Z0-9-_]+$"            # Alphanumeric, hyphens, underscores
    dbx_catalog: "^[a-zA-Z0-9_]+$"         # Alphanumeric and underscores only
    dbx_schema: "^[a-zA-Z0-9_]+$"          # Alphanumeric and underscores only
    dbx_table: "^[a-zA-Z0-9_]+$"           # Alphanumeric and underscores only
  
  # Required variables that must be present for each resource type
  # System will validate that these variables are available before name generation
  required_variables:
    aws_s3_bucket:
      - project
      - purpose
      - layer
      - environment
      - region_short
    
    aws_glue_database:
      - project
      - domain
      - layer
      - environment
    
    aws_glue_table:
      - entity
      # table_type is optional, defaults to 'fact'
    
    dbx_cluster:
      - project
      - workload
      - cluster_type
      - environment
    
    dbx_job:
      - project
      - job_type
      - purpose
      - environment
      # schedule is optional
    
    dbx_catalog:
      - project
      - catalog_type
      - environment
    
    dbx_schema:
      - domain
      - layer
    
    dbx_table:
      - entity
      # table_type is optional, defaults to 'fact'

# Usage Examples:
# ---------------
# 1. With defaults from naming-values.yaml:
#    dpn create --blueprint my-blueprint.json --values-config naming-values.yaml --patterns-config naming-patterns.yaml
#
# 2. Override specific values inline:
#    dpn create --blueprint my-blueprint.json --override project=oncology --override environment=prd
#
# 3. Use different patterns for specific environment:
#    dpn create --blueprint my-blueprint.json --patterns-config naming-patterns-oncology.yaml
#
# 4. Validate configuration files:
#    dpn config validate --values naming-values.yaml --patterns naming-patterns.yaml
